simple-llm-node â€” RAG tá»‘i giáº£n, cháº¡y Ä‘Æ°á»£c vá»›i Ollama hoáº·c Amazon Bedrock

Má»™t bá»™ khá»Ÿi táº¡o (starter) nhá» gá»n Ä‘á»ƒ báº¡n thá»­ RAG cá»¥c bá»™ vá»›i Node.js.
Há»— trá»£ 2 provider hoÃ¡n Ä‘á»•i qua .env: Ollama (cháº¡y táº¡i chá»—, GPU/CPU) vÃ  Amazon Bedrock (managed).

simple-llm-node/<br />
â”œâ”€ docs/<br />
â”‚  â””â”€ sample.md<br />
â”œâ”€ storage/<br />
â”‚  â””â”€ embeddings.json            # Táº¡o sau khi ingest<br />
â”œâ”€ src/<br />
â”‚  â”œâ”€ server.js                  # Express app + routes<br />
â”‚  â”œâ”€ rag.js                     # split, embed, cosine search<br />
â”‚  â”œâ”€ providers/<br />
â”‚  â”‚  â”œâ”€ bedrock.js              # chat + embeddings qua Bedrock<br />
â”‚  â”‚  â””â”€ ollama.js               # chat + embeddings qua Ollama<br />
â”‚  â””â”€ util.js                    # tiá»‡n Ã­ch chung<br />
â”œâ”€ .env.example<br />
â”œâ”€ package.json<br />
â””â”€ README.md<br />

TÃ­nh nÄƒng chÃ­nh

ğŸ” RAG pipeline tá»‘i giáº£n: Ä‘á»c Markdown â†’ chunk â†’ embedding â†’ lÆ°u embeddings.json â†’ cosine search â†’ augment context â†’ chat.

ğŸ”Œ Pluggable providers: chuyá»ƒn qua láº¡i Ollama / Bedrock báº±ng biáº¿n mÃ´i trÆ°á»ng (khÃ´ng Ä‘á»•i code).

ğŸ§ª CLI ingest + REST API gá»n nháº¹ Ä‘á»ƒ há»i/Ä‘Ã¡p vÃ  benchmark nhanh.

ğŸ–¥ï¸ (Tuá»³ chá»n) Gradio demo siÃªu nhanh Ä‘á»ƒ test UI.

YÃªu cáº§u

Node.js â‰¥ 18, Yarn

Quyá»n cháº¡y Ollama (náº¿u dÃ¹ng Ollama) hoáº·c quyá»n gá»i Bedrock (náº¿u dÃ¹ng Bedrock)

Windows / macOS / Linux Ä‘á»u OK

CÃ i Ä‘áº·t nhanh
# 1) Clone & cÃ i dependency
yarn install

# 2) Táº¡o file .env tá»« máº«u
cp .env.example .env

# 3) (Náº¿u dÃ¹ng Ollama) kÃ©o model chat + embedding
#   VÃ­ dá»¥ model chat: llama3.1, qwen2.5, deepseek-r1:8b (tuá»³ báº¡n)
#   VÃ­ dá»¥ model embed: nomic-embed-text:latest hoáº·c bge-m3
ollama pull llama3.1
ollama pull nomic-embed-text

# 4) Ingest tÃ i liá»‡u (máº·c Ä‘á»‹nh Ä‘á»c thÆ° má»¥c ./docs)
yarn run ingest

# 5) Cháº¡y server
yarn run dev


Tip: TrÃªn Windows, Ä‘áº£m báº£o Ollama Ä‘ang cháº¡y (dá»‹ch vá»¥ ná»n). Náº¿u ollama run bÃ¡o khÃ´ng tháº¥y model, hÃ£y ollama pull <model> trÆ°á»›c.

Cáº¥u hÃ¬nh .env

Sao chÃ©p tá»« .env.example vÃ  chá»‰nh phÃ¹ há»£p:

# Provider: 'ollama' hoáº·c 'bedrock'
PROVIDER=ollama

# --------- OLLAMA ----------
OLLAMA_BASE_URL=http://127.0.0.1:11434<br />
OLLAMA_CHAT_MODEL=llama3.1         # hoáº·c qwen2.5, deepseek-r1:8b, ...<br />
OLLAMA_EMBED_MODEL=nomic-embed-text # hoáº·c bge-m3, mxbai-embed-large, ...<br />

# --------- BEDROCK ---------
BEDROCK_REGION=us-east-1
BEDROCK_CHAT_MODEL=anthropic.claude-3-haiku-20240307-v1:0
BEDROCK_EMBED_MODEL=amazon.titan-embed-text-v1
# BEDROCK_MAX_TOKENS=1024           # tuá»³ chá»n: giá»›i háº¡n token Ä‘áº§u ra
# BEDROCK_TEMPERATURE=0.7           # tuá»³ chá»n: Ä‘iá»u chá»‰nh nhiá»‡t Ä‘á»™
# BEDROCK_SYSTEM_PROMPT="Báº¡n lÃ  trá»£ lÃ½ há»¯u Ã­ch" # tuá»³ chá»n: system prompt
# AWS_PROFILE=default               # náº¿u dÃ¹ng profile cá»¥c bá»™ (~/.aws/credentials)
# AWS_SDK_LOAD_CONFIG=1             # cáº§n báº­t khi dÃ¹ng profile

# --------- APP / RAG --------
PORT=3000<br />
DOCS_DIR=./docs<br />
EMBEDDINGS_PATH=./storage/embeddings.json<br />
CHUNK_SIZE=1000<br />
CHUNK_OVERLAP=120<br />
TOP_K=5<br />


Gá»£i Ã½ embedding Ä‘a ngÃ´n ngá»¯ (Ollama): bge-m3, multilingual-e5-large (náº¿u cÃ³), hoáº·c nomic-embed-text (tá»•ng quÃ¡t, nhanh).
Bedrock: titan-embed-text-v1/v2 lÃ  lá»±a chá»n cÃ¢n báº±ng cho Ä‘a ngÃ´n ngá»¯ phá»• biáº¿n.

Scripts tiá»‡n dá»¥ng (trong package.json)
{ 
  "scripts": {
    "dev": "node src/server.js",
    "ingest": "node src/server.js --ingest"
  }
}


yarn run ingest â†’ Ä‘á»c DOCS_DIR, cáº¯t Ä‘oáº¡n, táº¡o vector, lÆ°u EMBEDDINGS_PATH

yarn run dev â†’ má»Ÿ REST API

CÃ¡ch hoáº¡t Ä‘á»™ng (RAG pipeline)

Load táº¥t cáº£ *.md trong DOCS_DIR

Chunk theo CHUNK_SIZE, CHUNK_OVERLAP

Embed má»—i chunk báº±ng provider Ä‘Ã£ cáº¥u hÃ¬nh

Persist vÃ o EMBEDDINGS_PATH (gá»“m vectors + metadata)

LÃºc /api/ask:

Cosine search top K

Context compression (gá»™p + cáº¯t bá»›t)

Prompt augmentation â†’ gá»­i tá»›i chat model

Tráº£ vá» answer + citations (nguá»“n chunk)

REST API
Sá»©c khoáº»
GET /health


â†’ { status: "ok" }

Ingest láº¡i (náº¿u báº¡n cáº­p nháº­t docs)
POST /api/ingest
Body (tuá»³ chá»n): { "dir": "./docs" }

Há»i/ÄÃ¡p
POST /api/ask
Content-Type: application/json

{
  "question": "Qua 0h Ca2 thÃ¬ quy chiáº¿u ngÃ y nÃ o?",
  "topK": 5
}


VÃ­ dá»¥ cURL

curl -s http://localhost:3000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"NgÆ°á»¡ng R tá»‘i Ä‘a cá»§a BAT-12V-100Ah lÃ  bao nhiÃªu?"}'

Ingest tÃ i liá»‡u máº«u nhanh

Äáº·t file vÃ o docs/, vÃ­ dá»¥ docs/sample.md, hoáº·c dÃ¹ng bá»™ RAG Test Corpus â€” ACME báº¡n Ä‘Ã£ táº¡o.
Sau Ä‘Ã³:

yarn run ingest


Káº¿t quáº£ sáº½ xuáº¥t hiá»‡n trong storage/embeddings.json.

DÃ¹ng Ollama

CÃ i Ollama vÃ  cháº¡y daemon.

KÃ©o model chat vÃ  embed:

ollama pull llama3.1
ollama pull nomic-embed-text    # hoáº·c bge-m3


Kiá»ƒm tra nhanh:

ollama run llama3.1
ollama run nomic-embed-text


Äáº·t PROVIDER=ollama trong .env.

Lá»—i thÆ°á»ng gáº·p

Error: pull model manifest: file does not exist â†’ TÃªn model sai hoáº·c báº¡n Ä‘ang trá» tá»›i file cá»¥c bá»™ mÃ  chÆ°a táº¡o Modelfile.
VÃ­ dá»¥ dÃ¹ng file GGUF cá»¥c bá»™:

# Modelfile (cÃ¹ng thÆ° má»¥c .gguf)
FROM ./DeepSeek-R1-0528-Qwen3-8B-Q3_K_M.gguf


Rá»“i: ollama create mylocal-r1 -f Modelfile â†’ OLLAMA_CHAT_MODEL=mylocal-r1.

ollama run not working / khÃ´ng tháº¥y model â†’ ChÆ°a pull hoáº·c daemon chÆ°a cháº¡y.